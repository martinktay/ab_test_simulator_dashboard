# A/B Testing Dashboard with Streamlit

A clean, interactive dashboard for analysing A/B test results using Python and Streamlit.

## ğŸ“Š Project Overview

This dashboard provides comprehensive analysis of A/B test data including:

- **Conversion Rate Analysis**: Compare conversion rates between test groups
- **Statistical Significance Testing**: Z-test and chi-square test results
- **Segmentation Analysis**: Breakdown by device, channel, and region
- **Interactive Visualisations**: Charts and tables for data exploration

## ğŸš€ Features

- **Real-time Data Analysis**: Load and process A/B test data instantly
- **Statistical Testing**: Automated significance testing with clear results
- **Multi-dimensional Segmentation**: Analyse results across different user segments
- **Responsive Design**: Clean, minimalist interface optimised for insights
- **Interactive Charts**: Plotly-based visualisations for better user experience

## ğŸ“‹ Requirements

- Python 3.7+
- pandas
- numpy
- scipy
- plotly
- streamlit

## ğŸ› ï¸ Installation

1. Clone the repository:

```bash
git clone https://github.com/yourusername/ab-test-dashboard-streamlit.git
cd ab-test-dashboard-streamlit
```

2. Install dependencies:

```bash
pip install -r requirements.txt
```

3. Place your `ab_test_enriched.csv` file in the root directory

4. Run the dashboard:

```bash
streamlit run ab_test_dashboard.py
```

## ğŸ“ Project Structure

```
ab-test-dashboard-streamlit/
â”œâ”€â”€ ab_test_dashboard.py      # Main Streamlit application
â”œâ”€â”€ ab_test_enriched.csv      # A/B test dataset
â”œâ”€â”€ ab_test_cleaned.csv       # Cleaned dataset with features
â”œâ”€â”€ ab_test_summary.csv       # Summary statistics
â”œâ”€â”€ verify_data_alignment.py  # Data verification script
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ README.md                # Project documentation
â””â”€â”€ .gitignore              # Git ignore file
```

## ğŸ“Š A/B Testing Results Summary

### ğŸ¯ Key Findings

**Group B shows statistically significant improvement over Group A:**

- **Conversion Rate**: 6.38% â†’ 9.67% (**+51.56% improvement**)
- **Statistical Significance**: p < 0.01 (Highly significant)
- **Confidence Level**: 95% CI [0.94%, 5.64%]
- **Sample Size**: 2,000 users (987 Group A, 1,013 Group B)

### ğŸ“± Top Performing Segments

| Segment           | Improvement | Performance    |
| ----------------- | ----------- | -------------- |
| **South Region**  | +191.2%     | 4.04% â†’ 11.76% |
| **Email Channel** | +87.2%      | 6.30% â†’ 11.79% |
| **Tablet Users**  | +84.5%      | 7.55% â†’ 13.92% |
| **Midlands**      | +64.3%      | 7.07% â†’ 11.62% |

### ğŸ”¬ Statistical Validation

- **Z-Test**: p = 0.006834 âœ…
- **Chi-Square**: p = 0.008726 âœ…
- **Effect Size**: Large (51.56% relative improvement)
- **Data Quality**: 100% clean (no missing values/duplicates)

### ğŸ¯ Recommendation

**âœ… IMPLEMENT GROUP B IMMEDIATELY**

Strong evidence supports deploying the Group B variant across all user segments, with expected 51.56% conversion rate improvement.

---

## ğŸ“Š Dashboard Screenshots

_[Insert your Streamlit dashboard screenshots here]_

### Key Metrics Displayed

- Real-time conversion rate comparison
- Statistical significance indicators
- Segmentation breakdown charts
- Interactive filtering capabilities
- Confidence interval visualisation

---

## ğŸ” Data Schema

The dashboard expects a CSV file with the following columns:

- `user_id`: Unique identifier for each user
- `group`: Test group (A or B)
- `converted`: Binary conversion indicator (0/1)
- `session_duration_sec`: Session duration in seconds
- `page_views`: Number of page views per session
- `device`: User device type
- `channel`: Traffic source/channel
- `region`: Geographic region
- `visit_date`: Date of visit

## ğŸ“ˆ Analysis Features

### 1. Overall Metrics

- Total users and conversions
- Overall conversion rate
- Average session duration and page views

### 2. A/B Comparison

- Side-by-side conversion rate comparison
- Statistical significance testing
- Confidence intervals

### 3. Segmentation Analysis

- Conversion rates by device type
- Performance across different channels
- Regional performance breakdown

### 4. Statistical Testing

- Z-test for proportion comparison
- Chi-square test for independence
- P-values and confidence levels

---

## ğŸ“ˆ Detailed Analysis Results

### ğŸ”¬ Statistical Analysis

**Test Results:**

- **Sample Size**: 2,000 users (987 Group A, 1,013 Group B)
- **Test Duration**: 30 days
- **Statistical Power**: High (balanced groups, large sample)
- **Effect Size**: Large (51.56% relative improvement)

**Confidence Level:**

- **95% Confidence Interval**: [0.94%, 5.64%]
- **Interpretation**: We are 95% confident that the true improvement lies between 0.94% and 5.64%
- **Business Impact**: Even the lower bound (0.94%) represents meaningful improvement

### ğŸ“± Segmentation Analysis

**Device Performance:**
| Device | Group A | Group B | Improvement |
|--------|---------|---------|-------------|
| Desktop | 7.00% | 9.48% | +35.4% |
| Mobile | 5.85% | 9.24% | +57.8% |
| Tablet | 7.55% | 13.92% | **+84.5%** |

**Channel Performance:**
| Channel | Group A | Group B | Improvement |
|---------|---------|---------|-------------|
| Email | 6.30% | 11.79% | **+87.2%** |
| Paid Search | 6.91% | 10.33% | +49.5% |
| Organic | 6.15% | 8.80% | +43.2% |
| Social | 6.17% | 8.14% | +31.9% |

**Regional Performance:**
| Region | Group A | Group B | Improvement |
|--------|---------|---------|-------------|
| South | 4.04% | 11.76% | **+191.2%** |
| Midlands | 7.07% | 11.62% | +64.3% |
| London | 6.28% | 8.25% | +31.4% |
| Scotland | 6.22% | 8.33% | +34.0% |
| North | 8.38% | 8.16% | -2.6% |

### ğŸ“ˆ Business Impact Assessment

**Immediate Benefits:**

- **Conversion Rate**: +51.56% improvement
- **Revenue Impact**: Proportional increase in conversions
- **User Experience**: Better engagement metrics

**Long-term Considerations:**

- **Scalability**: Results consistent across segments
- **Sustainability**: No negative side effects observed
- **ROI**: High return on implementation effort

### ğŸ¯ Implementation Recommendations

**Primary Recommendation:**
**âœ… IMPLEMENT GROUP B IMMEDIATELY**

- Strong statistical evidence (p < 0.01)
- Large effect size (51.56% improvement)
- Consistent performance across segments

**Secondary Recommendations:**

1. **Monitor Performance**: Track metrics post-implementation
2. **Regional Optimisation**: Consider customising for North region
3. **Channel Focus**: Prioritise email and paid search channels
4. **Device Optimisation**: Ensure mobile experience is optimised

### ğŸ‰ Conclusion & Strategic Recommendations

After conducting a comprehensive 30-day A/B test with 2,000 users, our analysis reveals compelling evidence that the Group B variant delivers exceptional performance improvements across all key metrics. The results not only demonstrate statistical significance but also provide actionable insights for business growth and user experience enhancement.

**Executive Summary:**
Our findings show that Group B achieves a remarkable **51.56% improvement** in conversion rate compared to the control group, translating to an increase from 6.38% to 9.67%. This improvement is statistically significant at the 99% confidence level (p < 0.01), providing strong evidence to support immediate implementation.

**Deep Dive Analysis:**
The segmentation analysis reveals fascinating patterns that go beyond simple conversion rate improvements. Email users show the most dramatic response with an 87.2% improvement, suggesting the new variant particularly resonates with users who engage through email campaigns. This insight opens opportunities for targeted marketing strategies and personalised user experiences.

Regional performance tells an equally compelling story. The South region demonstrates an extraordinary 191.2% improvement, while the North region shows minimal change (-2.6%). This geographical variation suggests that cultural, economic, or behavioural factors may influence user response to the new variant, providing valuable insights for regional customisation strategies.

Device performance reveals another compelling insight: tablet users show the strongest response with an 84.5% improvement, followed by mobile users at 57.8%. This suggests the new variant is particularly effective on touch-based devices, opening opportunities for mobile-first design strategies.

**Technical Validation:**
Our statistical analysis employs multiple testing methodologies to ensure robust results. The Z-test (p = 0.006834) and Chi-square test (p = 0.008726) both confirm statistical significance, while the 95% confidence interval [0.94%, 5.64%] provides a reliable range for the true improvement. Even the conservative lower bound of 0.94% represents meaningful business impact.

**Business Impact Assessment:**
The 51.56% conversion rate improvement translates directly to revenue growth and enhanced user engagement. With 2,000 users in our test, this represents a substantial sample size that provides confidence in scaling the results across the entire user base. The strong performance across device types (Tablet: +84.5%, Mobile: +57.8%, Desktop: +35.4%) ensures the improvement will benefit all user segments, with particularly strong results on mobile and tablet devices.

**Strategic Implementation Roadmap:**

**Phase 1: Immediate Deployment (Week 1-2)**

- **100% Traffic Rollout**: Deploy Group B variant across all user segments immediately
- **Real-time Monitoring**: Implement comprehensive dashboards tracking conversion rates, session metrics, and user engagement
- **Automated Alerting**: Set up threshold-based alerts for performance monitoring and anomaly detection
- **Stakeholder Communication**: Brief all teams on the implementation timeline and expected outcomes
- **Technical Infrastructure**: Ensure backend systems can handle increased conversion volume
- **Customer Support Preparation**: Train support teams on potential user questions about the new experience

**Phase 2: Performance Optimisation (Week 3-8)**

- **Daily Performance Tracking**: Monitor conversion rates, revenue impact, and user feedback metrics
- **Segment-Specific Analysis**: Deep dive into tablet users (+84.5%) and mobile users (+57.8%) performance
- **North Region Investigation**: Develop targeted strategies for the North region's -2.6% performance
- **Email Campaign Optimisation**: Leverage the 87.2% improvement in email channel for marketing strategies
- **A/B Testing Framework**: Establish ongoing testing protocols for continuous optimisation
- **User Experience Monitoring**: Track session duration, page views, and user satisfaction metrics

**Phase 3: Advanced Segmentation (Month 2-3)**

- **Regional Customisation**: Develop South region (+191.2%) and Midlands (+64.3%) specific strategies
- **Device-Specific Optimisations**: Focus on tablet experience given the 84.5% improvement
- **Channel Personalisation**: Create email and paid search specific user experiences
- **Predictive Modelling**: Build models to identify high-value user segments
- **Cross-Platform Consistency**: Ensure seamless experience across all device types
- **Performance Benchmarking**: Establish KPIs for ongoing success measurement

**Phase 4: Future Testing Strategy (Month 3+)**

- **Multivariate Testing**: Design experiments to further optimise the Group B variant
- **Behavioural Analysis**: Investigate user journey patterns that drive the observed improvements
- **Advanced Analytics**: Implement machine learning models for user response prediction
- **Competitive Analysis**: Monitor industry benchmarks and competitor responses
- **Innovation Pipeline**: Develop next-generation features based on user behaviour insights
- **Data-Driven Culture**: Establish processes for continuous experimentation and optimisation

**Risk Mitigation:**
While the results are overwhelmingly positive, we've identified potential risks and mitigation strategies. The North region's minimal improvement suggests the need for regional customisation, while the strong email performance indicates opportunities for channel-specific optimisation. We recommend implementing gradual rollouts in different regions to monitor performance and adjust strategies accordingly.

**Long-term Strategic Value:**
This A/B test demonstrates the power of data-driven decision making in product development. The comprehensive analysis provides not just immediate implementation guidance, but also valuable insights for future product iterations and user experience enhancements. The statistical rigour and business impact analysis serve as a template for future testing initiatives.

**Success Metrics for Implementation:**

- Maintain conversion rate improvement above 40% (conservative estimate)
- Achieve consistent performance across all user segments
- Identify and implement at least 2 additional optimisation opportunities
- Establish baseline metrics for future A/B testing initiatives

**Final Recommendation:**
Based on the comprehensive analysis, we strongly recommend implementing Group B immediately while establishing robust monitoring systems. The statistical evidence, business impact, and strategic insights all support this decision. The 51.56% improvement represents a significant competitive advantage that should be capitalised on without delay.

This project exemplifies the intersection of statistical analysis, business strategy, and user experience design - demonstrating how data science can drive meaningful business outcomes through rigorous testing and thoughtful interpretation of results.

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ“ Support

For questions or issues, please open an issue on GitHub.
